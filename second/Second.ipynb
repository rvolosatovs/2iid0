{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "from enum import IntEnum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "\n",
    "from sklearn import tree, metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_csv(input_file, output_file):\n",
    "    df = pd.read_csv(input_file, header = 0, sep=',', thousands=',')\n",
    "    toScale = ['attr1_1','sinc1_1','intel1_1','fun1_1','amb1_1','shar1_1', \n",
    "           'attr2_1','sinc2_1','intel2_1','fun2_1','amb2_1','shar2_1', \n",
    "           'attr3_1','sinc3_1','intel3_1','fun3_1','amb3_1', \n",
    "           'attr4_1','sinc4_1','intel4_1','fun4_1','amb4_1','shar4_1', \n",
    "           'attr5_1','sinc5_1','intel5_1','fun5_1','amb5_1']\n",
    "    \n",
    "    def scaleAttrs(r):\n",
    "        for group in [toScale[0:6],toScale[6:12],toScale[12:17],toScale[17:23],toScale[23:28]]:\n",
    "            s = np.sum(r[group])\n",
    "            assert not s == 0 and not s == np.isnan(s)\n",
    "            r[group] = r[group]/s\n",
    "        return r\n",
    "    \n",
    "    df[toScale] = df[toScale].apply(scaleAttrs, axis=1)\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_pAge(df):\n",
    "    df = df.copy()\n",
    "    ages = df[['iid','age']].groupby(['iid']).mean()\n",
    "    df['pAge'] = df['pid'].apply(lambda x: math.nan if math.isnan(x) else ages.age[x]) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(X, verbose=False):\n",
    "    # Copy to avoid looping over the array we're modifying\n",
    "    cols = X.columns.values\n",
    "    for col in cols:\n",
    "        if X[col].dtypes=='object':\n",
    "            #print('Classifying {0}'.format(col))\n",
    "            X = X.drop(col, axis=1)\n",
    "            if verbose:\n",
    "                print('Dropping column {0}'.format(col))\n",
    "            # This is really heavy\n",
    "            #classes = X[col].str.get_dummies().rename(columns=lambda x: 'field-{0}'.format(x).replace(' ',''))\n",
    "            #X = pd.concat([X,classes])\n",
    "        elif X[col].dtypes=='float64' and X[col].isnull().values.any():\n",
    "            assert not col == 'iid' and not col == 'id' and not col == 'idg'\n",
    "            #print('Imputing {0}'.format(col))\n",
    "            # fill in missing values\n",
    "            if col == 'field_cd' or \\\n",
    "                col == 'gender' or \\\n",
    "                col == 'undergrd' or \\\n",
    "                col == 'race' or \\\n",
    "                col == 'from' or \\\n",
    "                col == 'career_c':\n",
    "                X[[col]]=Imputer(missing_values='NaN', strategy='most_frequent', axis=0).fit_transform(X[[col]])\n",
    "            else:\n",
    "                X[[col]]=Imputer(missing_values='NaN', strategy='mean', axis=0).fit_transform(X[[col]])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data \n",
    "def preprocess(df, verbose=False):\n",
    "    return impute(df.drop(columns=['iid', 'id', 'idg', 'condtn', 'wave', 'round', 'position', \n",
    "                            'positin1', 'order', 'partner', 'pid',\n",
    "                            'zipcode', # zipcode -> income \n",
    "                            #'undergra', -> {mn_sat, tuition} \n",
    "                            'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob',\n",
    "                            'match',\n",
    "                            #'gender', \n",
    "                            'you_call', 'them_cal', 'date_3', 'numdat_3', 'num_in_3',\n",
    "                           ], errors='ignore'), verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitBy(df, attr):\n",
    "    return df.drop(columns=[attr]), df[attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,y,test_size=0.2,random_state=0,min_samples_split=0.02, max_depth=10, accuracy_file=None, print_stats=True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier(min_samples_split=min_samples_split, max_depth=max_depth)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = clf.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_predict).ravel()/len(y_test)\n",
    "    if print_stats:\n",
    "        accuracy_str = \"\"\"Accuracy: {0:.2f}%\n",
    "True negatives: {1:.2f}%\\tFalse negatives: {2:.2f}%\n",
    "False positives: {3:.2f}%\\tTrue positives: {4:.2f}%\\n\"\"\".format(\n",
    "            accuracy*100, tn*100, fp*100, fn*100, tp*100)\n",
    "        print(accuracy_str)\n",
    "    \n",
    "    if not accuracy_file == None:\n",
    "        with open(accuracy_file, 'w') as f:\n",
    "            f.write(accuracy_str)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize(model, columns, out_file=None):\n",
    "    graph = graphviz.Source(\n",
    "        tree.export_graphviz(model, out_file=None,\n",
    "                                feature_names=columns,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True))\n",
    "    if not out_file == None:\n",
    "        graph.render(out_file)\n",
    "        \n",
    "    return graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gender(IntEnum):\n",
    "    FEMALE = 0;\n",
    "    MALE = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the attrs turned out to be really slow, so store preprocessed data.\n",
    "#convert_raw_csv(\"data.csv\", \"data_converted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_converted.csv\", header=0, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_pAge(df).rename({'age':'sAge'}, axis='columns').plot.hexbin(\n",
    "    x='sAge', y='pAge', C='match',\n",
    "    cmap=plt.cm.cool, \n",
    "    reduce_C_function=np.mean, \n",
    "    gridsize=22,\n",
    "    sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y = splitBy(preprocess(df), 'dec')\n",
    "X = X.drop(columns=['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_model = model(X, Y, test_size=0.2)\n",
    "vizualize(model(X, Y, test_size=0.2, max_depth=4, print_stats=False), X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nobias = impute(X.drop([\"race\",\"imprace\",\"imprelig\",\"income\"], axis=1))\n",
    "nobias_model = model(X_nobias,Y, test_size=0.2)\n",
    "vizualize(model(X_nobias,Y, max_depth=4, test_size=0.2, print_stats=False), X_nobias.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDiscriminationScore(attr, d_uni, d_nobias):\n",
    "    print(\"\"\"Discrimination score(slift) towards {0}:\n",
    " Unisex model: {1}\n",
    " No bias model: {2}\"\"\".format(attr, d_uni, d_nobias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminationScore(df):\n",
    "    means = df[['dec','nobias_dec','uni_dec']].mean()\n",
    "    d_uni = abs(means['dec']-means['uni_dec'])\n",
    "    d_nobias = abs(means['dec']-means['nobias_dec'])    \n",
    "    return d_uni, d_nobias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dec = impute(preprocess(df.copy()))\n",
    "\n",
    "df_dec['uni_dec'] = uni_model.predict(X.as_matrix())\n",
    "df_dec['nobias_dec'] = nobias_model.predict(X_nobias.as_matrix())\n",
    "\n",
    "print()\n",
    "\n",
    "### Pearson coefficent of correlation between attributes and decisions\n",
    "corr = df_dec.corr().drop(['uni_dec', 'nobias_dec', 'dec'])\n",
    "\n",
    "### Sorted by unisex model correlation\n",
    "uni_corr = corr[['uni_dec', 'nobias_dec']].sort_values(by='uni_dec')\n",
    "uni_corr.head(10).append(uni_corr.tail(10)).plot.bar()\n",
    "plt.show()\n",
    "\n",
    "### Sorted by no-bias model correlation\n",
    "nobias_corr = corr[['uni_dec', 'nobias_dec']].sort_values(by='nobias_dec')\n",
    "nobias_corr.head(10).append(nobias_corr.tail(10)).plot.bar()\n",
    "plt.show()\n",
    "\n",
    "printDiscriminationScore('gender', *discriminationScore(df_dec[df_dec.gender == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_equal = df_dec[df_dec.nobias_dec != df_dec.uni_dec]\n",
    "not_equal_mean = not_equal.mean().drop(['uni_dec', 'nobias_dec', 'dec'])\n",
    "df_dec_mean = df_dec.mean().drop(['uni_dec', 'nobias_dec', 'dec'])\n",
    "\n",
    "### Relative difference in means of attributes between general data set and cases, where model decisions differ\n",
    "not_equal_prop = (not_equal_mean/df_dec_mean).sort_values()\n",
    "not_equal_prop.head(5).append(not_equal_prop.tail(5)).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In the cases, where decision between model differs:\")\n",
    "print(\"Gender:\")\n",
    "not_equal.gender.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sports:\")\n",
    "not_equal.sports.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gaming:\")\n",
    "not_equal.gaming.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"Excercise:\")\n",
    "not_equal.exercise.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"Race:\")\n",
    "not_equal.race.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"amb1_1:\")\n",
    "not_equal.amb1_1.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"amb2_1:\")\n",
    "not_equal.amb2_1.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"amb4_1:\")\n",
    "not_equal.amb4_1.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"shopping:\")\n",
    "not_equal.shopping.value_counts().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"career_c\")\n",
    "not_equal.career_c.value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)\n",
    "\n",
    "### Pearson coefficent of correlation between gender and attributes\n",
    "corr = df.corr()['gender'].drop('gender').sort_values()\n",
    "corr.head(13).append(corr.tail(13)).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeElift(df, A, B, C):\n",
    "    return (df.query('{0} and {1} and {2}'.format(A,B,C)).count()/df.query('{0} and {1}'.format(B,C)).count())[0]\n",
    "\n",
    "elift = computeElift(df, 'amb1_1 > {0}'.format(df.amb1_1.mean()), 'age > 20 and age < 40', 'gender == {0}'.format(Gender.FEMALE))\n",
    "print('''Elift for\n",
    "A: amb1_1 > {0}\n",
    "B: 20 < age < 40\n",
    "C: gender == 0 (female)\n",
    "is: {1}'''.format(df.amb1_1.mean(), elift))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
